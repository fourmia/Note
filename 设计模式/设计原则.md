### 前言
此篇文章为自己了解了设计原则后，在青海多源融合项目上进行的一次小的尝试，设计上可能存在诸多问题，希望大家不吝指导。文章中首先就介绍了面向对象设计的五大原则，然后给出自己主观应用的具体案例
***
### SOLID 五大原则

    SOLIT是SOLID 是面向对象设计(OOD）五大基本原则的首字母缩写，分别为 S-单一职责原则，O-开放封闭原则，L-里氏替换原则，I-接口隔离原则，D-依赖倒置原则

#### 单一职责原则 SPR
+ 核心思想：一个类应该有且仅有一个引起他的变更


#### 开放封闭原则 OCP
+ 核心思想：软件实体应该对扩展开放，对修改封闭

#### 里氏替换原则 LSP
+ 核心思想：所有能引用基类的地方必须能透明的使用其子类的对象

#### 依赖倒置原则 DIP
+ 高层模块不应依赖低层模块， 二者都应依赖其抽象。抽象不应依赖细节， 细节应依赖抽象
  + 高层模块为调用方
  + 低层模块为具体实现类
  + 抽象是指接口或者抽象类
  + 细节是指具体实现类

#### 接口隔离原则 ISP
+ 客户端不应依赖它不需要的接口。用多个细粒度的接口来代替由多个方法组成的复杂接口，每一个接口服务一个子模块

***
下面使用青海多源融合短临系统的一个案例来进行说明:
+ 任务目标：完成多种模式预报与实况格点的数据匹配，用以进行模型训练,目前用到的模式数据包括EC、gfs、beijing、shanghai、meso
+ 任务要求：能灵活的扩展模式预报数据种类，又不会影响现有功能


> 需求分析如下：由于要根据当前实况时间选取最近的模式数据、分析场数据，因此需要匹配到各模式数据的最新起报时间文件，然后从预报序列中与当前实况格点文件时间进行筛选，进行数据匹配，完成特征构建。以降水为例，完成ec、gfs、beijing等模式预报与实况匹配后，应得到一个降水特征的二维数组，每一列为一种模式降水，与实况格点标记一一对应。

下面开始进行代码设计：从需求分析中可以看出，需要根据当前时间（也可是传入时间），完成模式数据和实况数据的选取，且要求易扩展，根据**依赖倒置原则**，**开放封闭原则**，设计数据匹配抽象类，该抽象类主要用以定义接口，即各模式、实况数据等子类生成的数据都应当是通过`get_data`方法调用得到的，代码中`DataMatch`为抽象/接口；`EcDataMatch`、`GfsDataMatch`...等为具体实现。
+ 接口
```python

class DataMatch(object):
    """Data matching base class, data matching based on information such as incoming time and path."""

    def __init__(self, ftime: Union[str, dt.datetime], filepath: str) -> None:
        """

        Args:
            ftime (Union[str, dt.datetime]): input time, str or dt.datetime format.
            filepath (str): mode or analysis grid file save path.
        """
        self.ftime = ftime
        self.filepath = filepath

    @abstractmethod
    def _select_data(self):
        raise NotImplementedError

    @abstractmethod
    def get_data(self):
        raise NotImplementedError
```
+ 具体实现
```python
class EcDataMatch(DataMatch):
    """EC data matching class"""
    def __init__(self, ftime: Union[str, dt.datetime], filepath: str, regex: str=None, begin_index=0, end_index=48):
        """

        Args:
            ftime (Union[str, dt.datetime]): ec report time, CST, str or dt.datetime format.
            filepath (str): ec mode  grid file save path.
            regex (str, optional): Regular expression to match files. If it is None, the built-in rules are used to match, Defaults to None.
            begin_index (int, optional): Start index subscript, left open right closed. Defaults to 0.
            end_index (int, optional):  End index subscript, left open right closed. Defaults to 48.
        """
        super().__init__(ftime, filepath)
        regex = '/*.nc' if regex is None else regex
        self.fregex = filepath + regex
        self.begin_index = begin_index
        self.end_index = end_index
        self.file_series, self.freq_series = self._select_data()

    def _select_data(self) -> Tuple[Tuple[str], Tuple[int]]:
        """data matching based on information such as incoming time and path.

        Returns:
            Tuple[Tuple[str], Tuple[int]]: (file_series, freq_series)-->(xxx.003.nc, xxx.006.nc,...), (3,6,...)
        """
        if isinstance(self.ftime, str):
            self.ftime = pd.to_datetime(self.ftime, format="%Y%m%d%H%M")
        fpath = self.ftime.strftime(self.fregex)
        file_list = sorted(glob.glob(fpath), key=lambda x: int(x.split('.')[-2][:3]))
        file_series = tuple(filter(lambda x: (int(x.split('.')[-2][:3])>self.begin_index and int(x.split('.')[-2][:3])<=self.end_index), file_list))
        freq_series = tuple(int(fname.split('.')[-2][:3]) for fname in file_series)
        return file_series, freq_series

    def get_data(self) -> xr.Dataset:
        """ get ecdata.

        Returns:
            xr.Dataset: ec predict dataset.
        """
        if len(self.file_series):
            dr_list = [ModeTimeInterp(fname, self.ftime, freq, gap=2).get_interp('RAIN03', 'RAIN01') for fname, freq in zip(self.file_series, self.freq_series)]
        else:
            logger.error("There is no data matching according to the current time, please check whether the EC file exists.")
            return
        return xr.concat(dr_list, dim='time').transpose("time","lat","lon")


class GfsDataMatch(DataMatch):
    """GFS data match class."""
    def __init__(self, ftime: Union[str, dt.datetime], filepath: str, regex: str=None, begin_index=0, end_index=48):
        """

        Args:
            ftime (Union[str, dt.datetime]): gfs report time, CST, str or dt.datetime format.
            filepath (str): ec mode  grid file save path.
            regex (str, optional): Regular expression to match files. If it is None, the built-in rules are used to match, Defaults to None.
            begin_index (int, optional): Start index subscript, left open right closed. Defaults to 0.
            end_index (int, optional):  End index subscript, left open right closed. Defaults to 48.
        """
        super().__init__(ftime, filepath)
        regex = '/*/*TPE*_1_2.grib2' if regex is None else regex
        self.fregex = filepath + regex
        self.begin_index = begin_index
        self.end_index = end_index
        self.file, self.freq_series= self._select_data()

    def _select_data(self):
        if isinstance(self.ftime, str):
            self.ftime = pd.to_datetime(self.ftime, format="%Y%m%d%H%M")
        fpath = self.ftime.strftime(self.fregex)
        file_list = glob.glob(fpath)
        if len(file_list) > 0:
            filter_list = sorted(filter(lambda x: int(os.path.split(x)[0][-2:]) < self.ftime.hour, file_list))
        else:
            raise ValueError(f"The path:{fpath} don't exist file, please check it.")
        return filter_list[-1], [3,6,9,12,15,18]

    def get_data(self):
        return ModeTimeInterp(self.file, self.ftime, 3, gap=2).get_interp('unknown', 'RAIN01')

class AnaDataMatch(DataMatch):
    """analysis data matching class"""
    def __init__(self, ftime: Union[str, dt.datetime], filepath: str, regex: str=None):
        """

        Args:
            ftime (Union[str, dt.datetime]):now time, CST, str or dt.datetime format, accurate to the hour.
            filepath (str): analysis grid file save path.
            regex (str, optional): Regular expression to match files. If it is None, the built-in rules are used to match, Defaults to None.
        """
        super().__init__(ftime, filepath)
        regex = f'/*FRT*_HOR-PRE-{self.ftime}.GRB2' if regex is None else regex
        self.fregex = self.filepath + regex
        self.fileseries = self._select_data()

    def _extract_time(self, fpath: str) -> str:
        # TODO: 针对测试所用的文件名生成，生产环境需要变更
        """from file abs path extract timestrings
        Args:
            fpath (str): file abs path, include time string.
        Returns:
            str: timestring, ex /f/2022/01/01/radar20220101080000.nc --> 202201080000
        """
        # time_strings = re.findall('(\d{8}.\d{4}00)', fpath)[0]
        time_strings = re.findall('(\d{10,})', fpath)[-1]
        # time_strings = re.findall(regex, fpath)[0]s
        return time_strings.replace('.', '')

    def _select_data(self) -> pd.DataFrame:
        """data matching based on information such as incoming time and path.

        Returns:
            pd.DataFrame: only one items, columns include [file_path, file_time].
        """
        #* 获取所有的文件名称
        if isinstance(self.ftime, str):
            self.ftime = pd.to_datetime(self.ftime, format="%Y%m%d%H")
        fpath = self.ftime.strftime(self.fregex)
        file_df = pd.DataFrame(tuple(sorted(glob.glob(fpath))), columns=['file_path'])
        file_df['file_time'] = pd.to_datetime(file_df.file_path.apply(self._extract_time), format="%Y%m%d%H")
        #* 获取所有的文件时间并得到最新文件item.
        sorted_df = file_df.sort_values('file_time')[-1:]
        return sorted_df

    def get_data(self) -> xr.Dataset:
        """usr xr and cfgrib get after cropping analysis rainfall grid.
        Returns:
            xr.Dataset:  cropped xarray.Dataset object.
        """
        filename = self.fileseries.file_path.values[0]
        dr = xr.open_mfdataset(filename, engine='cfgrib') if isinstance(filename, list) else xr.open_dataset(filename, engine='cfgrib')  #? 应注意这个只需要前十二个就好了，（可用于训练集和测试集输入构造）
        new_lat = np.round(np.arange(*SETTINGS_CONFIG['rain_model']['latlons']['lat']), 2)
        new_lon = np.round(np.arange(*SETTINGS_CONFIG['rain_model']['latlons']['lon']), 2)
        dr['latitude'] = np.round(dr.latitude.values, 2)
        dr['longitude'] = np.round(dr.longitude.values, 2)
        qh_dr = dr.sel(latitude=new_lat,longitude=new_lon)
        return qh_dr


......# 需要新增其它模式，新增一个具体子类即可
```
在具体实现的过程中`ModeTimeInterp`为参考刘红艳的时间序列插值代码实现，**主要功能是将模式插值到1H间隔**，由于gfs、ec数据组织形式问题，需分别实现；此处未作拆分，形成了脏代码，按个人理解来看抽象出一个插值类，被数据匹配类依赖,不过目前不影响整体阅读，了解其功能即可，还是给出其代码实现。
```python
class ModeTimeInterp(object):

    def __init__(self, filename: str, report_time: str, freq: int, gap: int, hres: str='1H'):
        """_summary_

        Args:
            filename (str): _description_
            report_time (str): _description_
            freq (int): _description_
            gap (int): _description_
            hres (str, optional): _description_. Defaults to '1H'.
        """
        self.report_time = report_time
        self.filename = filename
        self.freq = freq
        self.gap = gap
        self.hres = hres
        self.start_time = report_time + dt.timedelta(hours= freq-gap)
        self.end_time = report_time + dt.timedelta(hours = freq)

    def _interp_time_nc(self, ds):
        ft = pd.date_range(self.start_time, self.end_time, freq=self.hres)
        ds = ds.interp(time=np.array(ft), method='linear')
        return ds

    def _interp_time_grib(self, ds):
        ft = pd.date_range(ds.time.values, ds.valid_time.values[-1], freq=self.hres)
        newtime = np.array( [np.datetime64(t) - ds.time.values  for t in ft])
        ds = ds.interp(step=newtime, method='linear')
        ds['step'] = ft
        return ds

    def _deal_rain_nc(self, elem, newelem, ds_all):
        """针对逐小时、3小时累计值"""
        ds_0 = ds_all.sel(time=ds_all.time.values[0]).copy()
        ds_0['time'].values = np.datetime64(self.start_time)
        ds_0[elem].values = np.full(ds_0[elem].data.shape, 0)
        ds_all = xr.concat([ds_0, ds_all],dim='time')
        times = ds_all.time.values
        ds_all  = ds_all.cumsum(dim='time')
        ds_all = ds_all.assign_coords(time=times)
        ds_all = self._interp_time_nc(ds_all)
        ds_all = ds_all.diff(dim='time')
        ds_all = xr.concat([ds_0,ds_all], dim='time')
        ds_all = ds_all.rename({elem:newelem})
        ds_all[newelem].attrs = SETTINGS_CONFIG['rain_model']['attribute'][elem]
        ds_all= xr.where(ds_all>360, 360, ds_all) if self.hres =='3h' else xr.where(ds_all>120, 120, ds_all)
        ds_all = xr.where(ds_all<0.1,0, ds_all)
        return ds_all

    def _deal_rain_grib(self, elem, newelem, ds_all):
        ds_0 = ds_all.sel(step=ds_all.step.values[0]).copy()
        ds_0['step'].values = np.timedelta64(0, 'ns')
        ds_0[elem].values = np.full(ds_0[elem].data.shape, 0)
        ds_concat = xr.concat([ds_0, ds_all],dim='step')
        # times = ds_all.time.values
        steps = ds_concat.step.values
        ds_cumsum  = ds_concat.cumsum(dim='step')
        ds_new = ds_cumsum.assign_coords(step=steps)
        ds_new = ds_new.assign_coords(valid_time=ds_concat.valid_time)
        ds_all = self._interp_time_grib(ds_new)
        ds_diff = ds_all.diff(dim='step')
        ds_0_ = ds_0.copy()
        ds_0_['step'] = ds_0.time
        ds_0_ = ds_0_.drop('valid_time')
        ds_diff = xr.concat([ds_0_,ds_diff], dim='step')
        ds_diff = ds_diff.rename({elem:newelem})
        # ds_diff[newelem].attrs = SETTINGS_CONFIG['rain_model']['attribute'][elem]
        ds_diff= xr.where(ds_diff>360, 360, ds_diff) if self.hres =='3h' else xr.where(ds_diff>120, 120, ds_diff)
        ds_diff = xr.where(ds_diff<0.1,0, ds_diff)
        return ds_diff

    def get_interp(self, elem_name, new_elem_name):
        try:
            dr = xr.open_mfdataset(self.filename) if isinstance(self.filename, tuple) else xr.open_dataset(self.filename)
            ds = self._deal_rain_nc(elem_name, new_elem_name, dr)
        except ValueError:
            dr = xr.open_dataset(self.filename,engine='cfgrib')
            ds = self._deal_rain_grib(elem_name, new_elem_name, dr)
        return ds
```
至此完成了抽象和低层模块的编写，下面进行调用方代码编写，来完成特征构建工作,调用方主要根据传入的低层实例来生成特征，用以模型输入。
```python
class ModelDataset(object):

    def __init__(self):
        self.__modedata = []
        self.new_lat = np.round(np.arange(*SETTINGS_CONFIG['rain_model']['latlons']['lat']), 2)
        self.new_lon = np.round(np.arange(*SETTINGS_CONFIG['rain_model']['latlons']['lon']), 2)

    def addModeData(self, modedata):
        self.__modedata.append(modedata)

    def match_data(self, analysis_time) -> np.array:
        mode_list = []
        for modedata in self.__modedata:
            mode_dr = modedata.get_data()
            varname = list(mode_dr.data_vars.keys())[0]    #* 确保仅有一个 var
            if analysis_time is not None:
                try:
                    mode_pre = mode_dr.sel(time=analysis_time)
                except KeyError:
                    logger.error(f"real grid time:{analysis_time} not in {mode_dr.time.values}, program run filed!")
                except ValueError:
                    mode_pre = mode_dr.sel(step=analysis_time)
                try:
                    mode_interped = mode_pre.interp(lat=self.new_lat, lon=self.new_lon, kwargs={'fill_value':None})[varname].values.flatten()[:,np.newaxis]
                except ValueError:
                    mode_interped = mode_pre.interp(latitude=self.new_lat, longitude=self.new_lon, kwargs={'fill_value':None})[varname].values.flatten()[:,np.newaxis]
            else:
                try:
                    mode_interped = mode_dr.interp(lat=self.new_lat, lon=self.new_lon, kwargs={'fill_value':None})[varname].values.flatten()[:,np.newaxis]
                except ValueError:
                    mode_interped = mode_pre.interp(latitude=self.new_lat, longitude=self.new_lon, kwargs={'fill_value':None})[varname].values.flatten()[:,np.newaxis]
            mode_list.append(mode_interped)
        return np.concatenate(mode_list, axis=1)
```
由于在进行数据匹配时需要根据实况格点数据的时间来进行筛选，因此需要得到分析场数据并获取其时间作为匹配参数，具体使用过程如下
```python
if __name__ == '__main__':
    arg = Arg()
    args = arg.parse_args(args=None)
    now = dt.datetime.now().replace(second=0)
    SETTINGS_CONFIG["logpath"] = now.strftime(SETTINGS_CONFIG["logpath"])
    # mylogger = MyLogger(SETTINGS_CONFIG["logpath"], CONFIG, args.verbose)
    logger.info(args)
    #* 模型训练过程，无需考虑回算
    grid_time = args.time[:-2]    #? 实况时间

    ts_obj = GetTimeSeries(args.time, args.time)   #? EC起报时间
    report_time = ts_obj.get_report_series_08_20()[0]
    #* mode data
    ec_mode = EcDataMatch(report_time, filepath=SETTINGS_CONFIG['rain_model']['ecpath'])
    gfs_mode = GfsDataMatch(report_time, filepath=SETTINGS_CONFIG['rain_model']['cma_gfs'])
    bj_mode = EcDataMatch(report_time, filepath=SETTINGS_CONFIG['rain_model']['cma_bj'])
    sh_mode = EcDataMatch(report_time, filepath=SETTINGS_CONFIG['rain_model']['cma_sh9'])
    meso_mode = EcDataMatch(report_time, filepath=SETTINGS_CONFIG['rain_model']['cma_meso'])

    #* analysis_data
    analysis_obj = AnaDataMatch(grid_time, filepath=SETTINGS_CONFIG['rain_model']['analysis'])
    analysis_grid = analysis_obj.get_data()

    #* get mode data
    modeset = ModelDataset()
    mode_list = [ec_mode, gfs_mode, bj_mode, sh_mode, meso_mode]         #...所有需要新增的模式数据
    for mode in mode_list:
        modeset.addModeData(mode)
    model_input = modeset.match_data(analysis_grid.time.values)
```
测试结果如下图：

![设计模式-多源融合短临](https://raw.githubusercontent.com/fourmia/Picture/main/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%9A%E6%BA%90%E8%9E%8D%E5%90%88%E7%9F%AD%E4%B8%B4.png)


